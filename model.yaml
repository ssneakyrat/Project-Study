# Model Configuration
model:
  input_shape: [862, 80]
  output_shape: [22050]
  encoder_filters: [64, 128, 256, 512]
  decoder_filters: [256, 128, 64, 32]
  bottleneck_size: 2048
  dropout_rate: 0.3
  
# Training Configuration
training:
  batch_size: 64
  epochs: 200
  learning_rate: 0.0005
  lr_scheduler: true
  lr_factor: 0.5
  lr_patience: 5
  weight_decay: 0.0001
  grad_clip_val: 1.0
  grad_accumulation_steps: 1
  mixed_precision: true
  
  # Loss configuration
  use_envelope_loss: true
  use_multiscale_loss: true
  direct_loss_weight: 0.3
  envelope_loss_weight: 0.4
  multiscale_loss_weight: 0.3
  envelope_window_size: 128
  multiscale_scales: [1, 2, 4, 8]

# Data Configuration
data:
  data_file: "signal_data.h5"
  train_val_split: 0.8
  num_workers: 4
  persistent_workers: true
  prefetch_factor: 2
  log_scale_normalize: true

# Logging Configuration
logging:
  log_every_n_steps: 10
  log_val_every_epoch: 5
  tensorboard_log_dir: "logs"
  save_model_dir: "logs/checkpoints"
  save_every_n_epochs: 10
  save_best_models: true
  save_best_models_count: 5